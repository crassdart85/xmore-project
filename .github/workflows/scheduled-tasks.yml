name: Trading System Automation

on:
  schedule:
    # EGX daily snapshot export after market close (Sun-Thu, 14:00 UTC)
    # 14:00 UTC ~= 16:00/17:00 Cairo (post-close across DST)
    - cron: '0 14 * * 0-4'

    # EGX midday collection: 2:30 PM Cairo = 17:30 UTC (Sun-Thu)
    - cron: '30 17 * * 0-4'

    # Full daily pipeline: 5:00 PM EST = 22:00 UTC (Sun-Fri)
    # Runs AFTER both EGX (17:30) and US (included in collect_data.py) markets
    - cron: '0 22 * * 0-5'

    # Catchup evaluation: 3x daily (06:00, 12:00, 18:00 UTC)
    # Resolves predictions whose target_date has passed since last pipeline run
    - cron: '0 6,12,18 * * *'

  workflow_dispatch:

# Prevent overlapping pipeline runs
concurrency:
  group: trading-pipeline
  cancel-in-progress: false

env:
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
  NEWS_API_KEY: ${{ secrets.NEWS_API_KEY }}
  ALPHA_VANTAGE_API_KEY: ${{ secrets.ALPHA_VANTAGE_API_KEY }}

jobs:
  egx-daily-snapshot:
    if: github.event.schedule == '0 14 * * 0-4' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 25

    steps:
      - uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 1

      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements_data.txt

      - name: Run EGX daily snapshot job
        run: python xmore_data/daily_snapshot_job.py --interval 1d --output-dir data_exports/daily_egx_snapshots

      - name: Upload snapshot artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: egx-daily-snapshots-${{ github.run_id }}
          path: data_exports/daily_egx_snapshots/
          if-no-files-found: warn
  # ──────────────────────────────────────────────
  # Job 1: EGX midday collection (Sun-Thu 17:30 UTC)
  # Captures EGX close prices early so the main pipeline has fresh data
  # ──────────────────────────────────────────────
  egx-midday-collection:
    if: github.event.schedule == '30 17 * * 0-4' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 1

      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Initialize database schema
        run: python -c "from database import create_tables; create_tables()"

      - name: Collect EGX prices
        run: |
          python -c "
          from collect_data import collect_egx_data
          count = collect_egx_data()
          print(f'EGX collection complete: {count} stocks')
          "

      - name: Collect sentiment
        continue-on-error: true
        run: python sentiment.py

  # ──────────────────────────────────────────────
  # Job 2: Full daily pipeline (Sun-Fri 22:00 UTC)
  # Sequential: collect → sentiment → agents → portfolios → evaluate
  # On manual dispatch, waits for EGX collection to finish first
  # ──────────────────────────────────────────────
  daily-pipeline:
    needs: [egx-midday-collection]
    # Run even if egx-midday-collection was skipped (different schedule) or failed
    if: always() && (github.event.schedule == '0 22 * * 0-5' || github.event_name == 'workflow_dispatch')
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 1

      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Initialize database schema
        run: python -c "from database import create_tables; create_tables()"

      # Step 1: Collect all prices + news
      - name: Collect prices and news
        run: python collect_data.py

      # Step 2: Fetch custom news sources (URLs, RSS, Telegram channels)
      - name: Fetch custom news sources
        continue-on-error: true
        run: python engines/custom_source_fetcher.py --fetch-all

      # Step 3: Sentiment analysis (includes freshly-fetched custom source articles)
      - name: Analyze sentiment
        continue-on-error: true
        run: python sentiment.py

      # Step 4: Run 3-layer agent pipeline
      # (predictions → consensus → trade recs → briefing → performance eval)
      - name: Run agent pipeline
        run: python run_agents.py

      # Step 5: Generate model portfolios
      - name: Generate portfolios
        continue-on-error: true
        run: python engines/generate_portfolios.py

      # Step 6: Evaluate agent prediction accuracy (fills evaluations table)
      - name: Evaluate prediction accuracy
        run: python evaluate.py

  # ──────────────────────────────────────────────
  # Job 3: Catchup evaluation (3x daily)
  # Resolves predictions whose target_date has passed between pipeline runs
  # Lightweight: only evaluation, no data collection or predictions
  # ──────────────────────────────────────────────
  catchup-evaluation:
    if: github.event.schedule == '0 6,12,18 * * *'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 1

      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install -r requirements.txt

      # Agent prediction accuracy (evaluations table)
      - name: Evaluate prediction accuracy
        run: python evaluate.py

      # Trade performance metrics (returns, alpha, sharpe)
      - name: Evaluate trade performance
        run: python -c "from engines.evaluate_performance import run_evaluation; run_evaluation()"
